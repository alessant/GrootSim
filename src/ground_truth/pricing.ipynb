{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tiktoken\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "\n",
    "bayarea_sub_file = os.path.join(path, \"submissions\", \"bayarea_2020-07-01_2022-12-31\", \"bayarea_subs.csv\")\n",
    "california_sub_file = os.path.join(path, \"submissions\", \"California_2020-07-01_2022-12-31\", \"California_subs.csv\")\n",
    "collapse_sub_file = os.path.join(path, \"submissions\", \"collapse_2020-07-01_2022-12-31\", \"collapse_subs.csv\")\n",
    "news_sub_file = os.path.join(path, \"submissions\", \"news_2020-07-01_2022-12-31\", \"news_subs.csv\")\n",
    "politics_sub_file = os.path.join(path, \"submissions\", \"politics_2020-07-01_2022-12-31\", \"politics_subs.csv\")\n",
    "askreddit_sub_file = os.path.join(path, \"submissions\", \"AskReddit_2020-07-01_2022-12-31\", \"AskReddit_subs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_sub_df = pd.read_csv(bayarea_sub_file)\n",
    "california_sub_df = pd.read_csv(california_sub_file)\n",
    "collapse_sub_df = pd.read_csv(collapse_sub_file)\n",
    "news_sub_df = pd.read_csv(news_sub_file)\n",
    "politics_sub_df = pd.read_csv(politics_sub_file)\n",
    "askreddit_sub_df = pd.read_csv(askreddit_sub_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_to_include = pickle.load(open(os.path.join(path, \"to_include\", \"final_subs_to_include.pkl\"), \"rb\"))\n",
    "redditors_to_include = pickle.load(open(os.path.join(path, \"to_include\", \"final_users_to_include.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_to_include_no_prefix = [sub[3:] for sub in subs_to_include]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the subs and redditors to ignore\n",
    "bayarea_sub_df = bayarea_sub_df[bayarea_sub_df[\"sub_id\"].isin(subs_to_include_no_prefix)]\n",
    "california_sub_df = california_sub_df[california_sub_df[\"sub_id\"].isin(subs_to_include_no_prefix)]\n",
    "collapse_sub_df = collapse_sub_df[collapse_sub_df[\"sub_id\"].isin(subs_to_include_no_prefix)]\n",
    "news_sub_df = news_sub_df[news_sub_df[\"sub_id\"].isin(subs_to_include_no_prefix)]\n",
    "politics_sub_df = politics_sub_df[politics_sub_df[\"sub_id\"].isin(subs_to_include_no_prefix)]\n",
    "askreddit_sub_df = askreddit_sub_df[askreddit_sub_df[\"sub_id\"].isin(subs_to_include_no_prefix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181, 148, 2429, 385, 490, 2618)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bayarea_sub_df), len(california_sub_df), len(collapse_sub_df), len(news_sub_df), len(politics_sub_df), len(askreddit_sub_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the tokens needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_dir = os.path.join(path, \"threads\")\n",
    "subreddit_dirs = os.listdir(thread_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token counter utilities (since we need to be aware of their number)\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "#num_tokens = len(encoding.encode(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_tokens = 0\n",
    "subs_tokens_dict = {'askreddit':0, 'bayarea':0, 'california':0, 'collapse':0, 'news':0, 'politics':0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "# here we count the tokens we need to evaluate\n",
    "# submissions' titles and descriptions\n",
    "for subreddit in subreddit_dirs:\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    _path = os.path.join(os.path.dirname(eval(subreddit_name + \"_sub_file\")), 'text')\n",
    "    #print(_path)\n",
    "\n",
    "    _df = eval(subreddit_name + \"_sub_df\")\n",
    "\n",
    "    for index, row in _df.iterrows():\n",
    "        if pd.isnull(row[\"author_id\"]):\n",
    "            continue\n",
    "\n",
    "        if row[\"author_id\"] not in redditors_to_include:\n",
    "            continue\n",
    "\n",
    "        tokens = 0\n",
    "\n",
    "        sub_id = row[\"sub_id\"]\n",
    "        sub_title = row[\"sub_title\"].strip()\n",
    "        sub_desc = \"\"\n",
    "\n",
    "        desc_path = os.path.join(_path, sub_id + \".txt\")\n",
    "        with open(desc_path, \"r\") as f:\n",
    "            sub_desc = f.readlines()\n",
    "\n",
    "        sub_desc = \" \".join(sub_desc)\n",
    "\n",
    "        # print(sub_title)\n",
    "        # print(sub_desc)\n",
    "\n",
    "        if sub_title == sub_desc.strip():\n",
    "            tokens = len(encoding.encode(sub_title))\n",
    "        else:\n",
    "            tokens = len(encoding.encode(sub_title)) + len(encoding.encode(sub_desc))\n",
    "\n",
    "        subs_tokens += tokens\n",
    "        subs_tokens_dict[subreddit_name] += tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1106001,\n",
       " {'askreddit': 45365,\n",
       "  'bayarea': 47367,\n",
       "  'california': 3490,\n",
       "  'collapse': 994735,\n",
       "  'news': 4628,\n",
       "  'politics': 10416})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_tokens, subs_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.212002"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(subs_tokens / 1000) * 0.002"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "# here we add users from conversational threads\n",
    "for subreddit in subreddit_dirs:\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    subreddit_path = os.path.join(thread_dir, subreddit)\n",
    "    thread_files = os.listdir(subreddit_path)\n",
    "\n",
    "    for thread_file in thread_files:\n",
    "        if thread_file not in subs_to_include_no_prefix:\n",
    "            continue\n",
    "\n",
    "        thread_file_path = os.path.join(subreddit_path, thread_file, f\"{thread_file}.csv\")\n",
    "        comments_base_path = os.path.join(subreddit_path, thread_file, \"text\")\n",
    "        \n",
    "        try:\n",
    "            thread_df = pd.read_csv(thread_file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"Empty file:\", thread_file_path)\n",
    "            continue\n",
    "\n",
    "        if 'author_id' not in thread_df:\n",
    "            print(f\"Problems with {thread_file_path}\")\n",
    "            continue\n",
    "\n",
    "        for index, row in thread_df.iterrows():\n",
    "            if row[\"author_id\"] not in redditors_to_include:\n",
    "                continue\n",
    "\n",
    "            comment_path = os.path.join(comments_base_path, row[\"comm_id\"] + \".txt\")\n",
    "            comment = \"\"\n",
    "\n",
    "            with open(comment_path, \"r\") as f:\n",
    "                comment = f.readlines()\n",
    "            \n",
    "            comment = \" \".join(comment)\n",
    "\n",
    "            tokens = len(encoding.encode(comment))\n",
    "\n",
    "            subs_tokens += tokens\n",
    "            subs_tokens_dict[subreddit_name] += tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18097610,\n",
       " {'askreddit': 1368107,\n",
       "  'bayarea': 416564,\n",
       "  'california': 212651,\n",
       "  'collapse': 12908209,\n",
       "  'news': 1783612,\n",
       "  'politics': 1408467})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_tokens, subs_tokens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.19522"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(subs_tokens / 1000) * 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726.76612"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(subs_tokens / 1000) * 0.04"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
