{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_basepath = ''\n",
    "\n",
    "subs_input_path = os.path.join(input_basepath, 'submissions')\n",
    "threads_input_path = os.path.join(input_basepath, 'threads')\n",
    "\n",
    "subs_to_include_path = os.path.join(input_basepath, 'to_include', 'final_subs_to_include.pkl')\n",
    "users_to_include_path = os.path.join(input_basepath, 'to_include', 'final_users_to_include.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_to_include = pickle.load(open(subs_to_include_path, 'rb'))\n",
    "users_to_include = pickle.load(open(users_to_include_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_to_include_no_prefix = [sub[3:] for sub in subs_to_include]\n",
    "subs_to_include_no_prefix;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_basepath = ''\n",
    "subs_output_path = os.path.join(output_basepath, 'submissions')\n",
    "threads_output_path = os.path.join(output_basepath, 'threads')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AskReddit_2020-07-01_2022-12-31',\n",
       " 'California_2020-07-01_2022-12-31',\n",
       " 'news_2020-07-01_2022-12-31',\n",
       " 'collapse_2020-07-01_2022-12-31',\n",
       " 'bayarea_2020-07-01_2022-12-31',\n",
       " 'politics_2020-07-01_2022-12-31']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_dirs = os.listdir(subs_input_path)\n",
    "subreddit_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6251"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subs_to_include_no_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on AskReddit\n",
      "2738\n",
      "2618\n",
      "Working on California\n",
      "149\n",
      "148\n",
      "Working on news\n",
      "391\n",
      "385\n",
      "Working on collapse\n",
      "2430\n",
      "2429\n",
      "Working on bayarea\n",
      "184\n",
      "181\n",
      "Working on politics\n",
      "501\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "for subreddit_dir in subreddit_dirs:\n",
    "    subreddit_name = subreddit_dir.split(\"_\")[0]#.lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    subreddit_input_path = os.path.join(subs_input_path, subreddit_dir)\n",
    "    subreddit_output_path = os.path.join(subs_output_path, subreddit_dir)\n",
    "\n",
    "    if not os.path.exists(subreddit_output_path):\n",
    "        os.makedirs(subreddit_output_path)\n",
    "\n",
    "    # load the df containing the submissions' metadata\n",
    "    _df = pd.read_csv(os.path.join(subreddit_input_path, f'{subreddit_name}_subs.csv'))\n",
    "    print(len(_df))\n",
    "\n",
    "    # filter the df to only include the submissions that are in the list of submissions to include\n",
    "    df = _df[_df['sub_id'].isin(subs_to_include_no_prefix)]\n",
    "    print(len(df))\n",
    "\n",
    "    # store the filtered df\n",
    "    df.to_csv(os.path.join(subreddit_output_path, f'{subreddit_name}_subs.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter conversational threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on AskReddit\n",
      "2738\n",
      "2618\n",
      "Working on California\n",
      "149\n",
      "148\n",
      "Working on news\n",
      "391\n",
      "385\n",
      "Working on collapse\n",
      "2430\n",
      "2429\n",
      "Working on bayarea\n",
      "184\n",
      "181\n",
      "Working on politics\n",
      "501\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "for subreddit_dir in subreddit_dirs:\n",
    "    subreddit_name = subreddit_dir.split(\"_\")[0]\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    subreddit_input_path = os.path.join(threads_input_path, subreddit_dir)\n",
    "    thread_files = os.listdir(subreddit_input_path)\n",
    "\n",
    "    subreddit_output_path = os.path.join(threads_output_path, subreddit_dir)\n",
    "\n",
    "    if not os.path.exists(subreddit_output_path):\n",
    "        os.makedirs(subreddit_output_path)\n",
    "\n",
    "    print(len(thread_files))\n",
    "    cnt = 0\n",
    "\n",
    "    for thread_file in thread_files:\n",
    "        if thread_file not in subs_to_include_no_prefix:\n",
    "            continue\n",
    "\n",
    "        thread_file_path = os.path.join(subreddit_input_path, thread_file, f\"{thread_file}.csv\")\n",
    "\n",
    "        try:\n",
    "            _thread_df = pd.read_csv(thread_file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"Empty file:\", thread_file_path)\n",
    "            continue\n",
    "\n",
    "        if 'author_id' not in _thread_df:\n",
    "            print(f\"Problems with {thread_file_path}\")\n",
    "            continue\n",
    "\n",
    "        # filter out all authors that are not in the list of users to include\n",
    "        thread_df = _thread_df[_thread_df['author_id'].isin(users_to_include)]\n",
    "\n",
    "        if len(thread_df) == 0:\n",
    "            print(f\"Empty thread: {thread_file_path}\")\n",
    "            continue\n",
    "\n",
    "        # store the filtered df\n",
    "        thread_df.to_csv(os.path.join(subreddit_output_path, f\"{thread_file}.csv\"), index=False)\n",
    "        cnt += 1\n",
    "    \n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
