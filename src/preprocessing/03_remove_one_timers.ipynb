{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook removes all users having commented a single time across all datasets\n",
    "(only if their comment is a leaf in the conversation tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddittors_data = pickle.load(open('reddittors_data.pickle', 'rb'))\n",
    "reddittors_data_ids = pickle.load(open('redditors_data_ids.pickle', 'rb'))\n",
    "submitters_data_ids = pickle.load(open('submitters_data_ids.pickle', 'rb'))\n",
    "reddittors_ids_to_names = pickle.load(open('reddittors_ids_to_names.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "subs_to_include = pickle.load(open(os.path.join(path, \"to_include\", \"final_subs_to_include.pkl\"), \"rb\"))\n",
    "redditors_to_include = pickle.load(open(os.path.join(path, \"to_include\", \"users_to_include.pkl\"), \"rb\"))\n",
    "bots = pickle.load(open(os.path.join(path, \"to_include\", \"bots.pkl\"), \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove users if not present in the list of users to include\n",
    "to_remove = bots\n",
    "\n",
    "for user in reddittors_data.keys():\n",
    "    if user not in redditors_to_include:\n",
    "        to_remove.add(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in to_remove:\n",
    "    if user in reddittors_data:\n",
    "        del reddittors_data[user]\n",
    "    if user in reddittors_data_ids:\n",
    "        del reddittors_data_ids[user]\n",
    "    if user in submitters_data_ids:\n",
    "        del submitters_data_ids[user]\n",
    "    if user in reddittors_ids_to_names:\n",
    "        del reddittors_ids_to_names[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94536, 94536, 3788, 94537)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddittors_data), len(reddittors_data_ids), len(submitters_data_ids), len(reddittors_ids_to_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find one time poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_timers = set()\n",
    "one_timers_sub = {}\n",
    "\n",
    "for user in reddittors_data.keys():\n",
    "    n_actions = 0\n",
    "\n",
    "    subreddits = reddittors_data[user].keys()\n",
    "    #print(subreddits)\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        #print(reddittors_data[user][subreddit])\n",
    "        n_actions += sum(reddittors_data[user][subreddit])\n",
    "\n",
    "    #print(n_actions)\n",
    "\n",
    "    if n_actions == 1:\n",
    "        one_timers.add(user)\n",
    "        one_timers_sub[user] = reddittors_data_ids[user][list(subreddits)[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53772, 53772)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_timers), len(one_timers_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_to_check = set(one_timers_sub.values())\n",
    "len(submissions_to_check)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which one-timer has posted a leaf comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "thread_dir = os.path.join(path, \"threads\")\n",
    "subreddit_dirs = os.listdir(thread_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "# here we check whether a one-timer has posted a leaf comment\n",
    "# in that case, we can safely ignore their comment\n",
    "for subreddit in subreddit_dirs:\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    subreddit_path = os.path.join(thread_dir, subreddit)\n",
    "    thread_files = os.listdir(subreddit_path)\n",
    "\n",
    "    for thread_file in thread_files:\n",
    "        # if the submission is not in the list of submissions to check, we can skip it\n",
    "        if thread_file not in submissions_to_check:\n",
    "            continue\n",
    "\n",
    "        thread_file_path = os.path.join(subreddit_path, thread_file, f\"{thread_file}.csv\")\n",
    "        try:\n",
    "            thread_df = pd.read_csv(thread_file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"Empty file:\", thread_file_path)\n",
    "            continue\n",
    "\n",
    "        if 'author_id' not in thread_df:\n",
    "            print(f\"Problems with {thread_file_path}\")\n",
    "            continue\n",
    "\n",
    "        all_authors = set(thread_df['author_id'].values)\n",
    "        one_timers_in_thread = all_authors.intersection(one_timers)\n",
    "\n",
    "        for user in one_timers_in_thread:\n",
    "            # we need to check whether the user has posted a leaf comment\n",
    "            # this means that no other comment has its id as parent_id\n",
    "\n",
    "            # comm_id = 't1_' + thread_df[thread_df['author_id'] == user]['comm_id'].values[0]\n",
    "            # as_parent = thread_df[thread_df['parent_id'] == comm_id]\n",
    "            comm_id = thread_df[thread_df['author_id'] == user]['comm_id'].values[0]\n",
    "            parent_ids = set([p_id[3:] for p_id in thread_df['parent_id'].values])\n",
    "            \n",
    "            if comm_id in parent_ids:\n",
    "                # the user has not posted a leaf comment\n",
    "                # so we should remove it from the list of one-timers\n",
    "                one_timers.remove(user)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40020"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_timers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check which one-timer is a submitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "\n",
    "bayarea_sub_file = os.path.join(path, \"submissions\", \"bayarea_2020-07-01_2022-12-31\", \"bayarea_subs.csv\")\n",
    "california_sub_file = os.path.join(path, \"submissions\", \"California_2020-07-01_2022-12-31\", \"California_subs.csv\")\n",
    "collapse_sub_file = os.path.join(path, \"submissions\", \"collapse_2020-07-01_2022-12-31\", \"collapse_subs.csv\")\n",
    "news_sub_file = os.path.join(path, \"submissions\", \"news_2020-07-01_2022-12-31\", \"news_subs.csv\")\n",
    "politics_sub_file = os.path.join(path, \"submissions\", \"politics_2020-07-01_2022-12-31\", \"politics_subs.csv\")\n",
    "askreddit_sub_file = os.path.join(path, \"submissions\", \"AskReddit_2020-07-01_2022-12-31\", \"AskReddit_subs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_sub_df = pd.read_csv(bayarea_sub_file)\n",
    "california_sub_df = pd.read_csv(california_sub_file)\n",
    "collapse_sub_df = pd.read_csv(collapse_sub_file)\n",
    "news_sub_df = pd.read_csv(news_sub_file)\n",
    "politics_sub_df = pd.read_csv(politics_sub_file)\n",
    "askreddit_sub_df = pd.read_csv(askreddit_sub_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "# we should not remove one-timers that have posted a submission\n",
    "\n",
    "# author_id -> [sub_ids]\n",
    "submitters = {}\n",
    "\n",
    "for subreddit in subreddit_dirs:\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    _df = eval(subreddit_name + \"_sub_df\")\n",
    "    #print(len(_df))\n",
    "\n",
    "    for index, row in _df.iterrows():\n",
    "        \n",
    "        if pd.isnull(row[\"author_id\"]):\n",
    "            continue\n",
    "\n",
    "        sub_id = 't3_'+ row[\"sub_id\"]\n",
    "        author_id = row[\"author_id\"]\n",
    "\n",
    "        if author_id not in submitters:\n",
    "            submitters[author_id] = {sub_id}\n",
    "        else:\n",
    "            submitters[author_id].add(sub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3877"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "one_timers_cp = one_timers.copy()\n",
    "\n",
    "for user in one_timers_cp:\n",
    "    if user in submitters:\n",
    "        cnt += 1\n",
    "        one_timers.remove(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38923"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_timers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store one_timers with pickle\n",
    "path = \"\"\n",
    "pickle.dump(one_timers, open(os.path.join(path, \"one_timers.pkl\"), \"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for all submissions in which each user has contributed only to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "\n",
    "bayarea_sub_file = os.path.join(path, \"submissions\", \"bayarea_2020-07-01_2022-12-31\", \"bayarea_subs.csv\")\n",
    "california_sub_file = os.path.join(path, \"submissions\", \"California_2020-07-01_2022-12-31\", \"California_subs.csv\")\n",
    "collapse_sub_file = os.path.join(path, \"submissions\", \"collapse_2020-07-01_2022-12-31\", \"collapse_subs.csv\")\n",
    "news_sub_file = os.path.join(path, \"submissions\", \"news_2020-07-01_2022-12-31\", \"news_subs.csv\")\n",
    "politics_sub_file = os.path.join(path, \"submissions\", \"politics_2020-07-01_2022-12-31\", \"politics_subs.csv\")\n",
    "askreddit_sub_file = os.path.join(path, \"submissions\", \"AskReddit_2020-07-01_2022-12-31\", \"AskReddit_subs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_sub_df = pd.read_csv(bayarea_sub_file)\n",
    "california_sub_df = pd.read_csv(california_sub_file)\n",
    "collapse_sub_df = pd.read_csv(collapse_sub_file)\n",
    "news_sub_df = pd.read_csv(news_sub_file)\n",
    "politics_sub_df = pd.read_csv(politics_sub_file)\n",
    "askreddit_sub_df = pd.read_csv(askreddit_sub_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = bots.union(one_timers)\n",
    "redditors_to_include = {x for x in redditors_to_include if x not in to_remove}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55613"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redditors_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_subs_to_include = [sub_id[3:] for sub_id in subs_to_include]\n",
    "_subs_to_include;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "subs_to_ignore = set()\n",
    "subs_to_ignore_authors = set()\n",
    "subs_to_ignore_cnt = 0\n",
    "\n",
    "for subreddit in subreddit_dirs:\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    subreddit_path = os.path.join(thread_dir, subreddit)\n",
    "    thread_files = os.listdir(subreddit_path)\n",
    "\n",
    "    # first, we load all submissions' metadata \n",
    "    # within the subreddit\n",
    "    _df = eval(subreddit_name + \"_sub_df\")\n",
    "\n",
    "    for thread_file in thread_files:\n",
    "        if thread_file not in _subs_to_include:\n",
    "            continue\n",
    "\n",
    "        # we get the submitter_id\n",
    "        submitter_id = _df[_df['sub_id'] == thread_file]['author_id'].iloc[0]\n",
    "\n",
    "        thread_file_path = os.path.join(subreddit_path, thread_file, f\"{thread_file}.csv\")\n",
    "        try:\n",
    "            thread_df = pd.read_csv(thread_file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"Empty file:\", thread_file_path)\n",
    "            continue\n",
    "\n",
    "        if 'author_id' not in thread_df:\n",
    "            print(f\"Problems with {thread_file_path}\")\n",
    "            continue\n",
    "       \n",
    "        # filter out all users not in redditors_to_include\n",
    "        thread_df = thread_df[thread_df['author_id'].isin(redditors_to_include)]\n",
    "\n",
    "        if len(thread_df) == 0:\n",
    "            #print(\"Empty file:\", thread_file_path)\n",
    "            subs_to_ignore.add('t3_'+thread_file)\n",
    "            subs_to_ignore_authors.add(submitter_id)\n",
    "            subs_to_ignore_cnt += 1\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 19)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subs_to_ignore), len(subs_to_ignore_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6253"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# okay, now we have all conversations in which only one-timers have interacted\n",
    "# we need to remove these conversations from the dataset\n",
    "cleaned_subs_to_include = [sub_id for sub_id in subs_to_include if sub_id not in subs_to_ignore]\n",
    "len(cleaned_subs_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"\"\n",
    "# pickle.dump(subs_to_ignore, open(os.path.join(path, \"one_timer_subs.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: nan\n"
     ]
    }
   ],
   "source": [
    "one_timer_subs_authors = set()\n",
    "\n",
    "for author in subs_to_ignore_authors:\n",
    "    try:\n",
    "        n_cont = sum([n_actions for actions in (reddittors_data[author]).values() for n_actions in actions])\n",
    "    \n",
    "        if n_cont == 1:\n",
    "            one_timer_subs_authors.add(author)\n",
    "        \n",
    "        if n_cont == 2:\n",
    "            subs = list(submitters[author])\n",
    "            if subs[0] in subs_to_ignore and subs[1] in subs_to_ignore:\n",
    "                print(\"Both in subs_to_ignore:\", author)\n",
    "                one_timer_subs_authors.add(author)\n",
    "    except KeyError:\n",
    "        print(\"KeyError:\", author)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_timer_subs_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle one_timer_subs_authors\n",
    "path = \"\"\n",
    "pickle.dump(one_timer_subs_authors, open(os.path.join(path, \"one_timer_subs_authors.pkl\"), \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groot-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
