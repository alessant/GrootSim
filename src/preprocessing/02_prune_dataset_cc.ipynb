{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook prunes the filtered dataset considering only the largest connected component\n",
    "\n",
    "We build the (undirected) reply-based graph, and we compute the largest connected component\n",
    "\n",
    "    - we link two users u and v if v has replied to u.\n",
    "    - we only consider all submissions created by a user contained in the giant component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "\n",
    "bayarea_sub_file = os.path.join(path, \"submissions\", \"bayarea_2020-07-01_2022-12-31\", \"bayarea_subs.csv\")\n",
    "california_sub_file = os.path.join(path, \"submissions\", \"California_2020-07-01_2022-12-31\", \"California_subs.csv\")\n",
    "collapse_sub_file = os.path.join(path, \"submissions\", \"collapse_2020-07-01_2022-12-31\", \"collapse_subs.csv\")\n",
    "news_sub_file = os.path.join(path, \"submissions\", \"news_2020-07-01_2022-12-31\", \"news_subs.csv\")\n",
    "politics_sub_file = os.path.join(path, \"submissions\", \"politics_2020-07-01_2022-12-31\", \"politics_subs.csv\")\n",
    "askreddit_sub_file = os.path.join(path, \"submissions\", \"AskReddit_2020-07-01_2022-12-31\", \"AskReddit_subs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_sub_df = pd.read_csv(bayarea_sub_file)\n",
    "california_sub_df = pd.read_csv(california_sub_file)\n",
    "collapse_sub_df = pd.read_csv(collapse_sub_file)\n",
    "news_sub_df = pd.read_csv(news_sub_file)\n",
    "politics_sub_df = pd.read_csv(politics_sub_file)\n",
    "askreddit_sub_df = pd.read_csv(askreddit_sub_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build user and subs data dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_dir = os.path.join(path, \"threads\")\n",
    "subreddit_dirs = os.listdir(thread_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddittors_data = {}\n",
    "subs_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "# here we populate the reddittors_data and subs_data dictionary\n",
    "# based on submission data\n",
    "for subreddit in subreddit_dirs:\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    _df = eval(subreddit_name + \"_sub_df\")\n",
    "\n",
    "    for index, row in _df.iterrows():\n",
    "        if pd.isnull(row[\"author_id\"]):\n",
    "            continue\n",
    "\n",
    "        sub_id = 't3_'+ row[\"sub_id\"]\n",
    "\n",
    "        # populate reddittors_data\n",
    "        if row[\"author_id\"] not in reddittors_data:\n",
    "            reddittors_data[row[\"author_id\"]] = {\n",
    "                \"submissions\": {sub_id}\n",
    "            }\n",
    "        else:\n",
    "            reddittors_data[row[\"author_id\"]][\"submissions\"].add(sub_id)\n",
    "\n",
    "        # populate subs_data\n",
    "        if sub_id not in subs_data:\n",
    "            subs_data[sub_id] = {\n",
    "                \"reddittors\": {row[\"author_id\"]}\n",
    "            }\n",
    "        else:\n",
    "            subs_data[sub_id][\"reddittors\"].add(row[\"author_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3877"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddittors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "# here we add users from conversational threads\n",
    "for subreddit in subreddit_dirs:\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    subreddit_path = os.path.join(thread_dir, subreddit)\n",
    "    thread_files = os.listdir(subreddit_path)\n",
    "\n",
    "    for thread_file in thread_files:\n",
    "        thread_file_path = os.path.join(subreddit_path, thread_file, f\"{thread_file}.csv\")\n",
    "        try:\n",
    "            thread_df = pd.read_csv(thread_file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"Empty file:\", thread_file_path)\n",
    "            continue\n",
    "\n",
    "        if 'author_id' not in thread_df:\n",
    "            print(f\"Problems with {thread_file_path}\")\n",
    "            continue\n",
    "\n",
    "        for index, row in thread_df.iterrows():\n",
    "            # populate reddittors_data\n",
    "            if row[\"author_id\"] not in reddittors_data:\n",
    "                reddittors_data[row[\"author_id\"]] = {\n",
    "                    \"submissions\": {row[\"sub_id\"]},\n",
    "                }\n",
    "            else:\n",
    "                reddittors_data[row[\"author_id\"]][\"submissions\"].add(row[\"sub_id\"])\n",
    "\n",
    "            # populate subs_data\n",
    "            if row[\"sub_id\"] not in subs_data:\n",
    "                subs_data[row[\"sub_id\"]] = {\n",
    "                    \"reddittors\": {row[\"author_id\"]},\n",
    "                }\n",
    "            else:\n",
    "                subs_data[row[\"sub_id\"]][\"reddittors\"].add(row[\"author_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102115"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddittors_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve all submissions in which each user has contributed only to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function returning the number of submissions a reddittor has contributed to\n",
    "def get_n_contributions(reddittors_data, user_id):\n",
    "    return len(reddittors_data[user_id][\"submissions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "# let's check how many reddittors have contributed to 1 submission\n",
    "# we have 7 conversations for which the submitter has also participated in other submissions\n",
    "subs_to_ignore_one_time = set()\n",
    "n_to_remove = 0\n",
    "\n",
    "for subreddit in subreddit_dirs:\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    subreddit_path = os.path.join(thread_dir, subreddit)\n",
    "    thread_files = os.listdir(subreddit_path)\n",
    "\n",
    "    for thread_file in thread_files:\n",
    "        thread_file_path = os.path.join(subreddit_path, thread_file, f\"{thread_file}.csv\")\n",
    "        try:\n",
    "            thread_df = pd.read_csv(thread_file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"Empty file:\", thread_file_path)\n",
    "            continue\n",
    "\n",
    "        if 'author_id' not in thread_df:\n",
    "            print(f\"Problems with {thread_file_path}\")\n",
    "            continue\n",
    "\n",
    "        contributed_subs = set()\n",
    "        \n",
    "        for index, row in thread_df.iterrows():\n",
    "            user_id = row[\"author_id\"]\n",
    "            sub_id = row[\"sub_id\"]\n",
    "\n",
    "            contributed_subs.update(reddittors_data[user_id][\"submissions\"])\n",
    "                \n",
    "\n",
    "        # if all users have contributed only to this submission, we ignore it\n",
    "        if len(contributed_subs) == 1 and sub_id in contributed_subs:\n",
    "            n_to_remove += 1\n",
    "            subs_to_ignore_one_time.add('t3_'+thread_file)\n",
    "            #print(\"All users have only contributed to this submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subs_to_ignore_one_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the interaction network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_id -> author_id\n",
    "subs2author = {}\n",
    "\n",
    "# author_id -> [sub_ids]\n",
    "submitters = {}\n",
    "submitters_subreddits = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "# add reddittors as nodes\n",
    "# from submission data\n",
    "# we cannot just use the dict we built before\n",
    "# since we need to \n",
    "# - store which user has submitted which submission\n",
    "# - remove 1-time contributors\n",
    "\n",
    "#_ignored_subs = set()\n",
    "\n",
    "deleted_users_cnt = 0\n",
    "\n",
    "for subreddit in subreddit_dirs:\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    _df = eval(subreddit_name + \"_sub_df\")\n",
    "    #print(len(_df))\n",
    "\n",
    "    for index, row in _df.iterrows():\n",
    "        # if row[\"sub_id\"] in subs_to_ignore:\n",
    "        #     #print(\"Ignoring submission (1-time contributors)\", row[\"sub_id\"])\n",
    "        #     _ignored_subs.add(row[\"sub_id\"])\n",
    "        #     continue\n",
    "        \n",
    "        if pd.isnull(row[\"author_id\"]):\n",
    "            deleted_users_cnt += 1\n",
    "            continue\n",
    "\n",
    "        sub_id = 't3_'+ row[\"sub_id\"]\n",
    "        author_id = row[\"author_id\"]\n",
    "\n",
    "        G.add_node(author_id)\n",
    "        subs2author[sub_id] = author_id\n",
    "\n",
    "        if author_id not in submitters:\n",
    "            submitters[author_id] = {sub_id}\n",
    "            submitters_subreddits[author_id] = {subreddit_name}\n",
    "        else:\n",
    "            submitters[author_id].add(sub_id)\n",
    "            submitters_subreddits[author_id].add(subreddit_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "984"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleted_users_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3877"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3877"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the discrepancy between the number of nodes in the graph\n",
    "# and the number of reddittors in the dict is due to the fact\n",
    "# that we have removed submissions with only 1-time contributors\n",
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bayarea_2020-07-01_2022-12-31'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_dirs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on askreddit\n",
      "Working on california\n",
      "Working on news\n",
      "Working on collapse\n",
      "Working on bayarea\n",
      "Working on politics\n"
     ]
    }
   ],
   "source": [
    "# now we add users from conversational threads\n",
    "# and we link two users u and v if v has replied to u\n",
    "# here we add users from conversational threads\n",
    "for subreddit in subreddit_dirs:#[4:5]\n",
    "    subreddit_name = subreddit.split(\"_\")[0].lower()\n",
    "    print(\"Working on\", subreddit_name)\n",
    "\n",
    "    subreddit_path = os.path.join(thread_dir, subreddit)\n",
    "    thread_files = os.listdir(subreddit_path)\n",
    "\n",
    "    for thread_file in thread_files:\n",
    "        thread_file_path = os.path.join(subreddit_path, thread_file, f\"{thread_file}.csv\")\n",
    "        try:\n",
    "            thread_df = pd.read_csv(thread_file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"Empty file:\", thread_file_path)\n",
    "            continue\n",
    "\n",
    "        if 'author_id' not in thread_df:\n",
    "            print(f\"Problems with {thread_file_path}\")\n",
    "            continue\n",
    "\n",
    "        # if row['sub_id'] in subs_to_ignore:\n",
    "        #     print(\"Ignoring submission (1-time contributors)\", row['sub_id'])\n",
    "        #     continue\n",
    "\n",
    "        # we store the authors' and comments' ids\n",
    "        # because we need to link them later\n",
    "        aut_comms_df = thread_df[[\"author_id\", \"comm_id\"]]\n",
    "\n",
    "        for index, row in thread_df.iterrows():\n",
    "            # if the author is a submitter,\n",
    "            # we do not add any other node or edge\n",
    "            # if row['is_submitter']:\n",
    "            #     continue\n",
    "\n",
    "            # if the submitter of the submission is not in the graph\n",
    "            # it means that they have deleted their account\n",
    "            # so we do not consider them\n",
    "            # if row['sub_id'] not in subs2author:\n",
    "            #     continue\n",
    "\n",
    "            author_id = row[\"author_id\"]\n",
    "\n",
    "            # add the author as a node\n",
    "            # if the node already exists, nothing happens\n",
    "            G.add_node(author_id)\n",
    "\n",
    "            # add the edge between the author and the user they replied to\n",
    "            # if the edge already exists, nothing happens\n",
    "            parent_id = row[\"parent_id\"]\n",
    "\n",
    "            # prnt parent id in submissions of submitters\n",
    "\n",
    "            # if the parent is a submission\n",
    "            if parent_id.startswith('t3_'):\n",
    "                try:\n",
    "                    author_parent_id = subs2author[parent_id]\n",
    "                except KeyError:\n",
    "                    #print(\"Submission not found:\", thread_file_path)\n",
    "                    # if this happens, it means that the submitter has deleted their account\n",
    "                    # we just ignore it\n",
    "                    continue\n",
    "            else: # if the parent is a comment\n",
    "                comm_id = parent_id[3:]\n",
    "                try:\n",
    "                    author_parent_id = aut_comms_df[aut_comms_df[\"comm_id\"] == comm_id][\"author_id\"].values[0]\n",
    "                except IndexError:\n",
    "                    # if this happens, it means that the parent comment has been deleted\n",
    "                    # we just ignore it\n",
    "                    continue\n",
    "\n",
    "            G.add_edge(author_id, author_parent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102115, 214780)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we analyze the CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = list(nx.connected_components(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_sizes = [len(c) for c in cc]\n",
    "# sort cc_sizes in descending order\n",
    "cc_sizes.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort cc by size\n",
    "cc.sort(key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94537"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cc[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval subs to include (considering the giant component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc_users = cc[0]\n",
    "subs_to_include = set()\n",
    "\n",
    "for redditor in gc_users:\n",
    "    subs_to_include.update(reddittors_data[redditor]['submissions'])\n",
    "\n",
    "len(subs_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ''\n",
    "\n",
    "with open(os.path.join(data_dir, 'subs_to_include.pkl'), 'wb') as f:\n",
    "    pickle.dump(subs_to_include, f)\n",
    "\n",
    "with open(os.path.join(data_dir, 'users_to_include.pkl'), 'wb') as f:\n",
    "    pickle.dump(gc_users, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval subs to ignore (if we don't consider only the giant component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basically, we will ignore all users that are not in the giant component\n",
    "# this means that, in principle, we could safely ignore all\n",
    "# the submissions they have contributed to\n",
    "redditors_to_ignore = set()\n",
    "\n",
    "for component in cc[1:]:\n",
    "    for redditor in component:\n",
    "        redditors_to_ignore.add(redditor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102116, 7578, 94538)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes(), len(redditors_to_ignore), G.number_of_nodes() - len(redditors_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate how many submissions we can potentially ignore\n",
    "\n",
    "possible_subs_to_ignore = set()\n",
    "\n",
    "for redditor in redditors_to_ignore:\n",
    "    for sub_id in reddittors_data[redditor]['submissions']:\n",
    "        possible_subs_to_ignore.add(sub_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1606, 984, 5409, 6393)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(possible_subs_to_ignore), deleted_users_cnt, len(subs2author), len(subs2author) + deleted_users_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4787"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_subs = len(subs2author) + deleted_users_cnt\n",
    "tot_subs - len(possible_subs_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_to_consider = set()\n",
    "to_ignore = 0\n",
    "\n",
    "for redditor in reddittors_data.keys():\n",
    "    if redditor in redditors_to_ignore:\n",
    "        to_ignore += 1\n",
    "        continue\n",
    "\n",
    "    for sub_id in reddittors_data[redditor]['submissions']:\n",
    "        subs_to_consider.add(sub_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1492"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = subs_to_consider.intersection(possible_subs_to_ignore)\n",
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it may happen that we add a user to the graph\n",
    "# but we do not add any edge because they have only replied to deleted users\n",
    "subs_to_ignore = possible_subs_to_ignore.difference(subs_to_consider)\n",
    "len(subs_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6279"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subs2author) + deleted_users_cnt - len(subs_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = subs_to_ignore_one_time.intersection(subs_to_ignore)\n",
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in z:\n",
    "    print(sub, subs2author[sub], reddittors_data[subs2author[sub]]['submissions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving subs_to_ignore and redditors_to_ignore\n",
    "# to avoid recomputing them\n",
    "data_dir = '/home/aleant/coccons/Groot-py/data/california_wildfire_use_case/filtered_subs_and_threads/to_ignore'\n",
    "\n",
    "with open(os.path.join(data_dir, \"subs_to_ignore.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(subs_to_ignore, f)\n",
    "\n",
    "with open(os.path.join(data_dir, \"redditors_to_ignore.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(redditors_to_ignore, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for redditor in redditors_to_ignore:\n",
    "    if len(reddittors_data[redditor]['submissions']) > 1:\n",
    "        cnt += 1\n",
    "        #print(redditor, len(reddittors_data[redditor]['submissions']))\n",
    "cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
